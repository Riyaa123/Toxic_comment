{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d34cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf10fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(r'C:\\Users\\Riya\\Desktop\\results\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd5c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca04cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f23ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(r'C:\\Users\\Riya\\Desktop\\results\\test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a601076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fce0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0935ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be92234d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              False\n",
       "comment_text    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24c97b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               False\n",
       "comment_text     False\n",
       "toxic            False\n",
       "severe_toxic     False\n",
       "obscene          False\n",
       "threat           False\n",
       "insult           False\n",
       "identity_hate    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf34deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y = train[labels].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ab3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc58a803",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_PATTERNS = {\n",
    "    ' american ':\n",
    "        [\n",
    "            'amerikan'\n",
    "        ],\n",
    "\n",
    "    ' adolf ':\n",
    "        [\n",
    "            'adolf'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' hitler ':\n",
    "        [\n",
    "            'hitler'\n",
    "        ],\n",
    "\n",
    "    ' fuck':\n",
    "        [\n",
    "            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
    "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
    "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
    "            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
    "            'feck ', ' fux ', 'f\\*\\*', 'f**k','fu*k',\n",
    "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
    "        ],\n",
    "\n",
    "    ' ass ':\n",
    "        [\n",
    "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$',\n",
    "            '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
    "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s','a55', '@$$'\n",
    "        ],\n",
    "    ' ass hole ':\n",
    "        [\n",
    "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole', 'a**hole'\n",
    "        ],\n",
    "\n",
    "    ' bitch ':\n",
    "        [\n",
    "            'b[w]*i[t]*ch', 'b!tch',\n",
    "            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
    "            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h', 'b!tch', 'bi+ch', 'l3itch'\n",
    "        ],\n",
    "\n",
    "    ' bastard ':\n",
    "        [\n",
    "            'ba[s|z]+t[e|a]+rd'\n",
    "        ],\n",
    "\n",
    "    ' trans gender':\n",
    "        [\n",
    "            'transgender'\n",
    "        ],\n",
    "\n",
    "    ' gay ':\n",
    "        [\n",
    "            'gay'\n",
    "        ],\n",
    "\n",
    "    ' cock ':\n",
    "        [\n",
    "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
    "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
    "        ],\n",
    "    ' dick ':\n",
    "        [\n",
    "            ' dick[^aeiou]', 'deek', 'd i c k', 'dik'\n",
    "        ],\n",
    "\n",
    "    ' suck ':\n",
    "        [\n",
    "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
    "        ],\n",
    "\n",
    "    ' cunt ':\n",
    "        [\n",
    "            'cunt', 'c u n t'\n",
    "        ],\n",
    "\n",
    "    ' bull shit ':\n",
    "        [\n",
    "            'bullsh\\*t', 'bull\\$hit'\n",
    "        ],\n",
    "\n",
    "    ' homo sex ual':\n",
    "        [\n",
    "            'homosexual'\n",
    "        ],\n",
    "\n",
    "    ' jerk ':\n",
    "        [\n",
    "            'jerk'\n",
    "        ],\n",
    "\n",
    "    ' idiot ':\n",
    "        [\n",
    "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n",
    "                                                                                      'i d i o t'\n",
    "        ],\n",
    "    ' dumb ':\n",
    "        [\n",
    "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
    "        ],\n",
    "\n",
    "    ' shit ':\n",
    "        [\n",
    "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t', '$h1t'\n",
    "        ],\n",
    "\n",
    "    ' shit hole ':\n",
    "        [\n",
    "            'shythole'\n",
    "        ],\n",
    "\n",
    "    ' retard ':\n",
    "        [\n",
    "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
    "        ],\n",
    "\n",
    "    ' rape ':\n",
    "        [\n",
    "            ' raped'\n",
    "        ],\n",
    "\n",
    "    ' dumb ass':\n",
    "        [\n",
    "            'dumbass', 'dubass'\n",
    "        ],\n",
    "\n",
    "    ' ass head':\n",
    "        [\n",
    "            'butthead'\n",
    "        ],\n",
    "    ' sex ':\n",
    "        [\n",
    "            'sexy', 's3x', 'sexuality'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' nigger ':\n",
    "        [\n",
    "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
    "        ],\n",
    "\n",
    "    ' shut the fuck up':\n",
    "        [\n",
    "            'stfu', 'st*u'\n",
    "        ],\n",
    "\n",
    "    ' pussy ':\n",
    "        [\n",
    "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses', 'p*ssy'\n",
    "        ],\n",
    "\n",
    "    ' faggot ':\n",
    "        [\n",
    "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
    "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
    "        ],\n",
    "\n",
    "    ' mother fucker':\n",
    "        [\n",
    "            ' motha ', ' motha f', ' mother f', 'motherucker',\n",
    "        ],\n",
    "      ' whore ':\n",
    "        [\n",
    "            'wh\\*\\*\\*', 'w h o r e'\n",
    "        ],\n",
    "    ' fucking ':\n",
    "        [\n",
    "            'f*$%-ing'\n",
    "        ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59092542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text,remove_repeat_text=True, remove_patterns_text=True, is_lower=True):\n",
    "\n",
    "  if is_lower:\n",
    "    text=text.lower()\n",
    "    \n",
    "  if remove_patterns_text:\n",
    "    for target, patterns in RE_PATTERNS.items():\n",
    "      for pat in patterns:\n",
    "        text=str(text).replace(pat, target)\n",
    "\n",
    "  if remove_repeat_text:\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text) \n",
    "\n",
    "  text = str(text).replace(\"\\n\", \" \")\n",
    "  text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "  text = re.sub('[0-9]',\"\",text)\n",
    "  text = re.sub(\" +\", \" \", text)\n",
    "  text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c05c21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "train['comment_text']=train['comment_text'].apply(lambda x: clean_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7c8cc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d aww he matches this background colour i m seemingly stuck with thanks talk january utc '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train['comment_text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec8a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['comment_text']=test['comment_text'].apply(lambda x: clean_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c56374b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' dear welcome to wikipedia unfortunately using your e mail address as your username is not a good idea wikipedia content is extensively copied and the site itself is one of the most visited sites in the world any edit you make on wikipedia will have your username attached to it and using your email address will make you a tempting target for spammers we recommend that you change your username at wikipedia changing username in order to prevent abuse if you need any help simply contact me on my talk page or go to wikipedia help desk another option is to place on your own talk page and someone will come shortly to help remember to sign your posts on talk pages with four tildes again welcome at '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test['comment_text'][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "151c4a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train=train['comment_text']\n",
    "comments_test=test['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6fb8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93c41cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments_train[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e97439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train=list(comments_train)\n",
    "comments_test=list(comments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c352cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a50e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b00611df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(text, lemmatization=True):\n",
    "  output=\"\"\n",
    "  if lemmatization:\n",
    "    text=text.split(\" \")\n",
    "    for word in text:\n",
    "       word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\")\n",
    "       word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "       word3 = wordnet_lemmatizer.lemmatize(word2, pos = \"a\")\n",
    "       word4 = wordnet_lemmatizer.lemmatize(word3, pos = \"r\")\n",
    "       output=output + \" \" + word4\n",
    "  else:\n",
    "    output=text\n",
    "  \n",
    "  return str(output.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38417fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5f3374a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-d4e6009487cc>:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for line in tqdm_notebook(comments_train, total=159571):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45962605aa7240a2aba66fe691f35b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "lemmatized_train_data = [] \n",
    "\n",
    "for line in tqdm_notebook(comments_train, total=159571): \n",
    "    lemmatized_train_data.append(lemma(line))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08a664ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you sir be my hero any chance you remember what page that s on'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatized_train_data[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7c57b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-3f4013397682>:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for line in tqdm_notebook(comments_test, total=len(comments_test)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f6f105d1364e74b83bbc9695ff674b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemmatized_test_data = [] \n",
    "\n",
    "for line in tqdm_notebook(comments_test, total=len(comments_test)): \n",
    "    lemmatized_test_data.append(lemma(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1d15a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stopword_list=STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc628d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d85ea",
   "metadata": {},
   "source": [
    "import itertools\n",
    "import string\n",
    "from string import ascii_lowercase\n",
    "\n",
    "def iter_all_strings():\n",
    "    for size in itertools.count(1):\n",
    "        for s in itertools.product(ascii_lowercase, repeat=size):\n",
    "            yield \"\".join(s)\n",
    "\n",
    "dual_alpha_list=[]\n",
    "for s in iter_all_strings():\n",
    "    dual_alpha_list.append(s)\n",
    "    if s == 'zz':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592778a2",
   "metadata": {},
   "source": [
    "dual_alpha_list.remove('i')\n",
    "dual_alpha_list.remove('a')\n",
    "dual_alpha_list.remove('am')\n",
    "dual_alpha_list.remove('an')\n",
    "dual_alpha_list.remove('as')\n",
    "dual_alpha_list.remove('at')\n",
    "dual_alpha_list.remove('be')\n",
    "dual_alpha_list.remove('by')\n",
    "dual_alpha_list.remove('do')\n",
    "dual_alpha_list.remove('go')\n",
    "dual_alpha_list.remove('he')\n",
    "dual_alpha_list.remove('hi')\n",
    "dual_alpha_list.remove('if')\n",
    "dual_alpha_list.remove('is')\n",
    "dual_alpha_list.remove('in')\n",
    "dual_alpha_list.remove('me')\n",
    "dual_alpha_list.remove('my')\n",
    "dual_alpha_list.remove('no')\n",
    "dual_alpha_list.remove('of')\n",
    "dual_alpha_list.remove('on')\n",
    "dual_alpha_list.remove('or')\n",
    "dual_alpha_list.remove('ok')\n",
    "dual_alpha_list.remove('so')\n",
    "dual_alpha_list.remove('to')\n",
    "dual_alpha_list.remove('up')\n",
    "dual_alpha_list.remove('us')\n",
    "dual_alpha_list.remove('we')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f8014f",
   "metadata": {},
   "source": [
    "for letter in dual_alpha_list:\n",
    "    stopword_list.add(letter)\n",
    "print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa240ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b99b6",
   "metadata": {},
   "source": [
    "dual_alpha_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99652f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e348f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def remove_stopwords(text, remove_stop=True):\n",
    "      output = \"\"\n",
    "      if remove_stop:\n",
    "        text=text.split(\" \")\n",
    "        for word in text:\n",
    "          if word not in stopword_list:\n",
    "            output=output + \" \" + word\n",
    "      else :\n",
    "        output=text\n",
    "\n",
    "      return str(output.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3151a3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-77ed00981e90>:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for line in tqdm_notebook(lemmatized_train_data, total=159571):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5666afe06de74da68e61aa1c334a5c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_train_data = [] \n",
    "\n",
    "for line in tqdm_notebook(lemmatized_train_data, total=159571): \n",
    "    processed_train_data.append(remove_stopwords(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b20825db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agree graemel s intention revert nazi wikipedia ha way protect report wp rr revert rule notice board post report revert nazi graemel ha revert nazi unacceptable site s user s admins feel unjustly revert time hour period report revert noticeboard wp rr time'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train_data[152458]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b906730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-9a1104780151>:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for line in tqdm_notebook(lemmatized_test_data, total=153164):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1604f7e0374f46b2bb8e9db591367b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_test_data = [] \n",
    "\n",
    "for line in tqdm_notebook(lemmatized_test_data, total=153164): \n",
    "    processed_test_data.append(remove_stopwords(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0319a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=100000      \n",
    "maxpadlen = 200          \n",
    "val_split = 0.2      \n",
    "embedding_dim_fasttext = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2532c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72962169",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(processed_train_data))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(processed_train_data)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(processed_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cca888b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in Vocabulary:  149326\n"
     ]
    }
   ],
   "source": [
    "word_index=tokenizer.word_index\n",
    "print(\"Words in Vocabulary: \",len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1197b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d343df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t=pad_sequences(list_tokenized_train, maxlen=maxpadlen, padding = 'post')\n",
    "X_te=pad_sequences(list_tokenized_test, maxlen=maxpadlen, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad4a2a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Tokenized sentences: \\n', X_t[10])\n",
    "#print('One hot label: \\n', y[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63149094",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(X_t.shape[0])\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a69a1b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = X_t[indices]\n",
    "labels = y[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2603d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = int(val_split*X_t.shape[0])\n",
    "x_train = X_t[: -num_validation_samples]\n",
    "y_train = labels[: -num_validation_samples]\n",
    "x_val = X_t[-num_validation_samples: ]\n",
    "y_val = labels[-num_validation_samples: ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c197a596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in each category:\n",
      "training:  [12311  1290  6795   382  6348  1138]\n",
      "validation:  [2983  305 1654   96 1529  267]\n"
     ]
    }
   ],
   "source": [
    "print('Number of entries in each category:')\n",
    "print('training: ', y_train.sum(axis=0)) \n",
    "print('validation: ', y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46fe5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "   embeddings_index_fasttext = {}\n",
    "f = open('C:/Users/Riya/Desktop/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embeddings_index_fasttext[word] = np.asarray(values[1:], dtype='float32')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59d1b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import talos\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcb85188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Completed!\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_fasttext = np.random.random((len(word_index) + 1, embedding_dim_fasttext))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index_fasttext.get(word)\n",
    "    if embedding_vector is   not None:\n",
    "        embedding_matrix_fasttext[i] = embedding_vector\n",
    "print(\" Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "30d38eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc61f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def toxic_classifier(x_train,y_train,x_val,y_val,params):\n",
    "\n",
    "  inp=Input(shape=(maxpadlen, ),dtype='int32')\n",
    "\n",
    "  embedding_layer = Embedding(len(word_index) + 1, \n",
    "                           embedding_dim_fasttext,\n",
    "                           weights = [embedding_matrix_fasttext],\n",
    "                           input_length = maxpadlen,\n",
    "                           trainable=False,\n",
    "                           name = 'embeddings')\n",
    "  embedded_sequences = embedding_layer(inp)\n",
    "\n",
    "  x = LSTM(params['output_count_lstm'], return_sequences=True,name='lstm_layer')(embedded_sequences)\n",
    "\n",
    "  x = Conv1D(filters=params['filters'], kernel_size=params['kernel_size'], padding='same', activation='relu', kernel_initializer='he_uniform')(x)\n",
    "\n",
    "  x = MaxPooling1D(params['pool_size'])(x)\n",
    "  \n",
    "  x = GlobalMaxPool1D()(x)\n",
    "  \n",
    "  x = BatchNormalization()(x)\n",
    "  \n",
    "  x = Dense(params['output_1_count_dense'], activation=params['activation'], kernel_initializer='he_uniform')(x)\n",
    "  \n",
    "  x = Dropout(params['dropout'])(x)\n",
    "\n",
    "  x = Dense(params['output_2_count_dense'], activation=params['activation'], kernel_initializer='he_uniform')(x)\n",
    "  \n",
    "  x = Dropout(params['dropout'])(x)\n",
    "  \n",
    "  preds = Dense(6, activation=params['last_activation'], kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "  model = Model(inputs=inp, outputs=preds)\n",
    "\n",
    "  model.compile(loss=params['loss'], optimizer=params['optimizer'], metrics=['accuracy'])\n",
    "\n",
    "  model_info=model.fit(x_train,y_train, epochs=params['epochs'], batch_size=params['batch_size'],  validation_data=(x_val, y_val))\n",
    "\n",
    "  return model_info, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ee16b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=toxic_classifier\n",
    "#model.fit(x_train,y_train)\n",
    "#filename=r'C:\\Users\\Riya\\Desktop\\toxic_proj\\pickl.pkl'\n",
    "#pickle.dump(scan_results,open(filename,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cc226f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=pickel.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p={\n",
    "    'output_count_lstm': [50,60],\n",
    "    'output_1_count_dense': [40,50],\n",
    "    'output_2_count_dense': [30,40],\n",
    "    'filters' : [64],\n",
    "    'kernel_size' : [3],\n",
    "    'batch_size': [32],\n",
    "    'pool_size': [3],\n",
    "    'epochs':[2],\n",
    "    'optimizer':['adam'],\n",
    "    'activation':['relu'],\n",
    "    'last_activation': ['sigmoid'],\n",
    "    'dropout':[0.1,0.2],\n",
    "    'loss': ['binary_crossentropy']   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920057ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_results = talos.Scan(x=x_train,\n",
    "               y=y_train,\n",
    "               x_val=x_val,\n",
    "               y_val=y_val,  \n",
    "               model=toxic_classifier,\n",
    "               params=p,\n",
    "               experiment_name='tcc',\n",
    "               print_params=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e98de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = scan_results.data['val_accuracy'].astype('float').argmax()\n",
    "model_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7201980",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=Input(shape=(maxpadlen, ),dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                           embedding_dim_fasttext,\n",
    "                           weights = [embedding_matrix_fasttext],\n",
    "                           input_length = maxpadlen,\n",
    "                           trainable=False,\n",
    "                           name = 'embeddings')\n",
    "embedded_sequences = embedding_layer(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdfd6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LSTM(50, return_sequences=True,name='lstm_layer')(embedded_sequences)\n",
    "x = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_uniform')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(40, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(30, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(6, activation=\"sigmoid\", kernel_initializer='glorot_uniform')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41482caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Model(inputs=inp, outputs=preds)\n",
    "model_2.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5f8093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_info_2=model_2.fit(x_train,y_train, epochs=2, batch_size=32,  validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db65a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.save(filepath=r\"C:\\Users\\Riya\\Desktop\\toxic_proj\\pickl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "058a9a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_2 = keras.models.load_model(filepath=r\"C:\\Users\\Riya\\Desktop\\toxic_proj\\pickl\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45b14b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 2038s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "test_values_2 = loaded_model_2.predict([X_te], batch_size=1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43d7980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(r'C:\\Users\\Riya\\Desktop\\results\\sample_submission.csv')\n",
    "test_values_2=pd.DataFrame(test_values_2,columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n",
    "submission = pd.DataFrame(sample_submission[\"id\"])\n",
    "combined_submission=pd.concat([submission,test_values_2],axis=1)\n",
    "combined_submission.to_csv('File_Path', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a31bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def toxicity_level(string):\n",
    "    new_string = [string]\n",
    "    new_string = tokenizer.texts_to_sequences(new_string)\n",
    "    new_string = pad_sequences(new_string, maxlen=maxpadlen, padding='post')\n",
    "    \n",
    "    prediction = model_2.predict(new_string) \n",
    "    print(\"Toxicity levels for '{}':\".format(string))\n",
    "    print('Toxic:         {:.0%}'.format(prediction[0][0]))\n",
    "    print('Severe Toxic:  {:.0%}'.format(prediction[0][1]))\n",
    "    print('Obscene:       {:.0%}'.format(prediction[0][2]))\n",
    "    print('Threat:        {:.0%}'.format(prediction[0][3]))\n",
    "    print('Insult:        {:.0%}'.format(prediction[0][4]))\n",
    "    print('Identity Hate: {:.0%}'.format(prediction[0][5]))\n",
    "    print()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051ac10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b4da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
